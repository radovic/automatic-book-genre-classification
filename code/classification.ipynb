{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d640a050",
   "metadata": {},
   "source": [
    "# Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18803d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of packages\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fca9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edaa79",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce637077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# reading the dataset \n",
    "df = pd.read_csv('../data/dataset_filtered_labels.csv')\n",
    "\n",
    "# getting the list of genres \n",
    "genres = set()\n",
    "for v in df['genres'].values: genres = set(list(genres) + ast.literal_eval(v))\n",
    "genres = list(genres)\n",
    "\n",
    "# creating the mappings from genres to id and vice versa\n",
    "genre2id = {k:v for (v, k) in enumerate(genres)}\n",
    "id2genre = {k:v for (k, v) in enumerate(genres)}\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8939cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = df['summary'].to_numpy() # corpus - predictor variables\n",
    "Y = np.full((X.shape[0], len(genres)), 0, dtype=int) # genres - target variables\n",
    "\n",
    "# populating Y\n",
    "\n",
    "genre_data = df['genres'].to_numpy() # genres assigned to works\n",
    "for idx in range(len(Y)):\n",
    "    genre_data[idx] = ast.literal_eval(genre_data[idx])\n",
    "    for g in genre_data[idx]: Y[idx][genre2id[g]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93457536",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Search of optimal value for min_df\n",
    "\n",
    "for val in [0.00375, 0.00625, 0.0075, 0.00875]:\n",
    "    opt_X_train, opt_X_test, opt_Y_train, opt_Y_test = train_test_split(X, Y, random_state=2023)\n",
    "    vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=val)\n",
    "    base_twcnb = ComplementNB()\n",
    "    clf_twcnb = OneVsRestClassifier(base_twcnb)\n",
    "    opt_X_train = vectorizer.fit_transform(opt_X_train)\n",
    "    opt_X_test = vectorizer.transform(opt_X_test)\n",
    "    clf_twcnb.fit(opt_X_train, opt_Y_train)\n",
    "    print(f'{val * 100: .4f}% -> {clf_twcnb.score(opt_X_test, opt_Y_test)}')\n",
    "\n",
    "# 0.000% -> 0.060659186535764374\n",
    "# 0.125% -> 0.3159186535764376 < OPTIMAL >\n",
    "# 0.250% -> 0.28856942496493687\n",
    "# 0.375% -> 0.258765778401122\n",
    "# 0.500% -> 0.22300140252454417\n",
    "# 0.625% -> 0.20196353436185133\n",
    "# 0.750% -> 0.21608598962194217\n",
    "# 0.875% -> 0.20570793180133431\n",
    "# 1.000% -> 0.15252454417952313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6800f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# defining tokenizer which performs lemmatization and skips stop or/and non-alphabetic words\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stops = set(stopwords.words('english'))\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t).lower() for t in word_tokenize(doc) if self.wnl.lemmatize(t) not in self.stops and t.isalpha()]\n",
    "    \n",
    "# defining TF-IDF vectorizer\n",
    "# we put threshold of 1% for term presence in summaries so we can filter out the least common terms\n",
    "# which can cause the overfitting of the classifier (this choice is backed by Zipf's law)\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=0.00125)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285269a4",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c739555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate, KFold\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, hamming_loss\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import randint\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644fb614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "def report_performance_on_test_sets(scores):\n",
    "    import numpy as np\n",
    "    \n",
    "    print('F1 micro score: mean = %.3f, standard deviation = %.3f' %(np.mean(scores['test_F1 micro']), \n",
    "                                                                 np.std(scores['test_F1 micro'])))\n",
    "    \n",
    "    print('Accuracy: mean = %.3f, standard deviation = %.3f' %(np.mean(scores['test_Accuracy']), \n",
    "                                                                 np.std(scores['test_Accuracy'])))\n",
    "    \n",
    "    print('Hamming loss: mean = %.3f, standard deviation = %.3f' %(np.mean(scores['test_Hamming loss']), \n",
    "                                                                 np.std(scores['test_Hamming loss'])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4790138",
   "metadata": {},
   "source": [
    "### TWCNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade44e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.now() \n",
    "\n",
    "base_twcnb = ComplementNB()\n",
    "clf_twcnb = OneVsRestClassifier(base_twcnb)\n",
    "\n",
    "steps = list()\n",
    "steps.append(('tfidf', TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=0.00125)))\n",
    "steps.append(('model', clf_twcnb))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "\n",
    "scoring = {\"F1 micro\": \"f1_micro\", \"Accuracy\": make_scorer(accuracy_score), \n",
    "           \"Hamming loss\": make_scorer(hamming_loss)}\n",
    "\n",
    "mnb_scores = cross_validate(pipeline, X=X, y=Y, cv=cv, scoring = scoring, return_estimator = True)\n",
    "\n",
    "print(\"Time taken to perform multiple model evaluation: \",datetime.now()-st)\n",
    "report_performance_on_test_sets(mnb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3271ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/scores/redgenres/TWCNB_scores_red.pkl', 'wb') as f:\n",
    "#    pickle.dump(twcnb_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25762dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/scores/allgenres/TWCNB_scores.pkl', 'rb') as g:\n",
    "    twcnb_scores = pickle.load(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4038e7",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_model(model, param_space, X, Y):\n",
    "    \n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('tfidf', TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=0.00125)))\n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    inner_cv = KFold(n_splits=2, shuffle=True, random_state=2023) # for hyperparameter tuning\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "\n",
    "    #defining scoring for k fold cross validation\n",
    "    scoring = {\"F1 micro\": \"f1_micro\", \"Accuracy\": make_scorer(accuracy_score), \n",
    "               \"Hamming loss\": make_scorer(hamming_loss)}\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    # for  hypoparameter tuning f1 score will be used to find the best parameters for refitting the \n",
    "    # estimator at the end. It is specified with the parameter refit = 'F1 micro'.\n",
    "    clf = RandomizedSearchCV(pipeline, param_space, n_iter=3, scoring = scoring, cv = inner_cv, \n",
    "                             refit = 'F1 micro')\n",
    "    \n",
    "    scores = cross_validate(clf, X=X, y=Y, cv=outer_cv, return_estimator = True, scoring = scoring)\n",
    "\n",
    "    # report performance\n",
    "    report_performance_on_test_sets(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7644b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.now() \n",
    "\n",
    "clf_lr = OneVsRestClassifier(LogisticRegression(max_iter = 200))\n",
    "param_space = {\n",
    "              'model__estimator__C':[0.01,0.1,1,5,10] \n",
    "              }\n",
    "lr_scores = tune_and_evaluate_model(clf_lr, param_space, X, Y)\n",
    "\n",
    "print(\"Time taken to perform hyperparameter tuning and multiple model evaluation: \",datetime.now()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21baf3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/scores/redgenres/LR_scores_red.pkl', 'wb') as g:\n",
    "#    pickle.dump(lr_scores, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44090f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/scores/allgenres/LR_scores.pkl', 'rb') as g:\n",
    "    lr_scores = pickle.load(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b94f1",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "Inherently supports mulitilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.now() \n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 2)\n",
    "\n",
    "param_space = {\n",
    "              'model__max_depth':list(np.arange(10, 250, step=50))\n",
    "              }\n",
    "rf_scores = tune_and_evaluate_model(rf, param_space, X, Y)\n",
    "print(\"Time taken to perform hyperparameter tuning and multiple model evaluation: \",datetime.now()-st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/scores/redgenres/RF_scores_red.pkl', 'wb') as h:\n",
    "#    pickle.dump(rf_scores, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/scores/allgenres/RF_scores.pkl', 'rb') as h:\n",
    "    rf_scores = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac05ede5",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87888496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "st=datetime.now() \n",
    "\n",
    "xgboost = xgb.XGBClassifier(objective = 'multi:softmax',                                  \n",
    "                            seed = 2023,  \n",
    "                            num_class=2,\n",
    "                            gamma =  0.1,\n",
    "                            learning_rate = 0.5,\n",
    "                            n_estimators = 200\n",
    "                            \n",
    "                         ) \n",
    "one_vs_rest_xgboost = OneVsRestClassifier(xgboost)\n",
    "\n",
    "param_space = {\n",
    "              'model__estimator__max_depth':[2, 5]\n",
    "              }\n",
    "xgboost_scores = tune_and_evaluate_model(one_vs_rest_xgboost, param_space, X, Y)\n",
    "print(\"Time taken to perform hyperparameter tuning and multiple model evaluation: \",datetime.now()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54be0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('../data/scores/redgenres/XGBoost_scores_red.pkl', 'wb') as i:\n",
    "#    pickle.dump(xgboost_scores, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d73d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/scores/allgenres/XGBoost_scores.pkl', 'rb') as i:\n",
    "    xgboost_scores = pickle.load(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab69fbee",
   "metadata": {},
   "source": [
    "## Evaluation of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3925fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(['./mplstyles/science.mplstyle', './mplstyles/nature.mplstyle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting box plots for scores\n",
    "\n",
    "models_scores_f1 = {'TWCNB': mnb_scores['test_F1 micro'], \n",
    "                    'LR': lr_scores['test_F1 micro'],\n",
    "                    'RF': rf_scores['test_F1 micro'],\n",
    "                    'XGBoost' : xgboost_scores['test_F1 micro']\n",
    "                   }\n",
    "models_scores_accuracy = {'TWCNB': mnb_scores['test_Accuracy'], \n",
    "                          'LR': lr_scores['test_Accuracy'],\n",
    "                          'RF': rf_scores['test_Accuracy'],\n",
    "                          'XGBoost' : xgboost_scores['test_Accuracy']\n",
    "                         }\n",
    "models_scores_hamming = {'TWCNB': mnb_scores['test_Hamming loss'], \n",
    "                         'LR': lr_scores['test_Hamming loss'],\n",
    "                         'RF': rf_scores['test_Hamming loss'],\n",
    "                         'XGBoost' : xgboost_scores['test_Hamming loss']\n",
    "                        }\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "fig.subplots_adjust(right=1.5, wspace=0.5)\n",
    "fig.suptitle(f'Models\\' performances on 15 genres', y=0.95, x=0.75)\n",
    "\n",
    "ax1.boxplot(models_scores_f1.values(), showmeans=True, meanline = True)\n",
    "ax1.set_xticklabels(models_scores_f1.keys())\n",
    "ax1.set_xlabel(\"Model\")\n",
    "ax1.set_ylabel(\"F1 (micro) score\")\n",
    "\n",
    "ax2.boxplot(models_scores_accuracy.values(), showmeans=True, meanline = True)\n",
    "ax2.set_xticklabels(models_scores_accuracy.keys())\n",
    "ax2.set_xlabel(\"Model\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax3.boxplot(models_scores_hamming.values(), showmeans=True, meanline = True)\n",
    "ax3.set_xticklabels(models_scores_hamming.keys())\n",
    "ax3.set_xlabel(\"Model\")\n",
    "ax3.set_ylabel(\"Hamming loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a212c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plottin F1 scores\n",
    "from math import sqrt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xbar = np.arange(len(models_scores_f1.keys()))\n",
    "ybar = [v.mean() for v in models_scores_f1.values()]\n",
    "stdev = [np.std(v) for v in models_scores_f1.values()]\n",
    "ax.bar(xbar, ybar, yerr=stdev, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_ylabel('F1 score')\n",
    "ax.set_xticks(xbar)\n",
    "ax.set_xticklabels(models_scores_f1.keys())\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('../gfx/allgenres_f1.jpg', dpi=300)\n",
    "\n",
    "print('Standard errors:', np.array(stdev) / sqrt(len(list(models_scores_f1.values())[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69df05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining standard errors\n",
    "\n",
    "f1s = np.array([np.std(v) for v in models_scores_f1.values()]) / sqrt(len(list(models_scores_f1.values())[0]))\n",
    "accs =  np.array([np.std(v) for v in models_scores_accuracy.values()]) / sqrt(len(list(models_scores_accuracy.values())[0]))\n",
    "hls = np.array([np.std(v) for v in models_scores_hamming.values()]) / sqrt(len(list(models_scores_hamming.values())[0]))\n",
    "\n",
    "print('Standard errors')\n",
    "for i in range(len(f1s)):\n",
    "    print('F1:', round(f1s[i], 4))\n",
    "    print('Acc:', round(accs[i], 4))\n",
    "    print('HL:', round(hls[i], 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51f1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting fitting times\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "X = list(range(len(mnb_scores['fit_time'])))\n",
    "Ys = [mnb_scores['fit_time'], lr_scores['fit_time'], rf_scores['fit_time'], xgboost_scores['fit_time']]\n",
    "\n",
    "models = ['TWCNB', 'LR', 'RF', 'XGBoost']\n",
    "xbar = np.arange(len(models))\n",
    "ybar = [v.mean() for v in Ys]\n",
    "stdev = [np.std(v) for v in Ys]\n",
    "ax.bar(xbar, ybar, yerr=stdev, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.set_xlabel('Model')\n",
    "# ax.set_ylabel('Mean fit time [s]')\n",
    "ax.set_xticks(xbar)\n",
    "ax.set_xticklabels(models_scores_f1.keys())\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('../gfx/allgenrestimes.jpg', dpi=300)\n",
    "\n",
    "print('Standard errors:', np.array(stdev) / sqrt(len(list(models_scores_f1.values())[0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
