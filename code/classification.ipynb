{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe2bd06",
   "metadata": {},
   "source": [
    "# Classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1e1f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of packages\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68770765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation of NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbf03f",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99e8fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading data\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# reading the dataset \n",
    "df = pd.read_csv('../data/dataset_filtered_labels.csv')\n",
    "\n",
    "# getting the list of genres \n",
    "genres = set()\n",
    "for v in df['genres'].values: genres = set(list(genres) + ast.literal_eval(v))\n",
    "genres = list(genres)\n",
    "\n",
    "# creating the mappings from genres to id and vice versa\n",
    "genre2id = {k:v for (v, k) in enumerate(genres)}\n",
    "id2genre = {k:v for (k, v) in enumerate(genres)}\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bfcf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = df['summary'].to_numpy() # corpus - predictor variables\n",
    "Y = np.full((X.shape[0], len(genres)), 0, dtype=int) # genres - target variables\n",
    "\n",
    "# populating Y\n",
    "\n",
    "genre_data = df['genres'].to_numpy() # genres assigned to works\n",
    "for idx in range(len(Y)):\n",
    "    genre_data[idx] = ast.literal_eval(genre_data[idx])\n",
    "    for g in genre_data[idx]: Y[idx][genre2id[g]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search of optimal value for min_df\n",
    "\n",
    "for val in [0, 0.000625, 0.00125, 0.001875, 0.0025, 0.005, 0.01]:\n",
    "    opt_X_train, opt_X_test, opt_Y_train, opt_Y_test = train_test_split(X, Y, random_state=2023)\n",
    "    vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=val)\n",
    "    base_twcnb = ComplementNB()\n",
    "    clf_twcnb = OneVsRestClassifier(base_twcnb)\n",
    "    opt_X_train = vectorizer.fit_transform(opt_X_train)\n",
    "    opt_X_test = vectorizer.transform(opt_X_test)\n",
    "    clf_twcnb.fit(opt_X_train, opt_Y_train)\n",
    "    print(f'{val * 100: .2f}% -> {clf_twcnb.score(X_test, Y_test)}')\n",
    "\n",
    "# 0% -> 0.060659186535764374\n",
    "# 0.0625% -> 0.2699859747545582\n",
    "# 0.125% -> 0.3159186535764376 < OPTIMAL >\n",
    "# 0.1875% -> 0.3075035063113605\n",
    "# 0.25% -> 0.28856942496493687\n",
    "# 0.5% -> 0.22300140252454417\n",
    "# 1.0% -> 0.15252454417952313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718bae91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# defining tokenizer which performs lemmatization and skips stop or/and non-alphabetic words\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stops = set(stopwords.words('english'))\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t).lower() for t in word_tokenize(doc) if self.wnl.lemmatize(t) not in self.stops and t.isalpha()]\n",
    "    \n",
    "# defining TF-IDF vectorizer\n",
    "# we put threshold of 1% for term presence in summaries so we can filter out the least common terms\n",
    "# which can cause the overfitting of the classifier (this choice is backed by Zipf's law)\n",
    "vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=0.00125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe3450",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate, KFold\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, hamming_loss\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import randint\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cc909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "def report_performance_on_test_sets(scores):\n",
    "    import numpy as np\n",
    "    \n",
    "    print('F1 micro score: mean = %.3f, standard deviation = %.3f' %(np.mean(scores['test_F1 micro']), \n",
    "                                                                 np.std(scores['test_F1 micro'])))\n",
    "    \n",
    "    print('Accuracy: mean = %.3f, standard deviation = %.3f' %(np.mean(scores['test_Accuracy']), \n",
    "                                                                 np.std(scores['test_Accuracy'])))\n",
    "    \n",
    "    print('Hamming loss: mean = %.3f, standard deviation = %.3f' %(np.mean(scores['test_Hamming loss']), \n",
    "                                                                 np.std(scores['test_Hamming loss'])))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f634a",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c32e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.now() \n",
    "\n",
    "# defining the model and fitting\n",
    "base_twcnb = ComplementNB()\n",
    "clf_twcnb = OneVsRestClassifier(base_twcnb)\n",
    "\n",
    "steps = list()\n",
    "steps.append(('tfidf', TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=0.00125)))\n",
    "steps.append(('model', clf_twcnb))\n",
    "pipeline = Pipeline(steps=steps)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "\n",
    "scoring = {\"F1 micro\": \"f1_micro\", \"Accuracy\": make_scorer(accuracy_score), \n",
    "           \"Hamming loss\": make_scorer(hamming_loss)}\n",
    "\n",
    "mnb_scores = cross_validate(pipeline, X=X, y=Y, cv=cv, scoring = scoring, return_estimator = True)\n",
    "\n",
    "print(\"Time taken to perform multiple model evaluation: \",datetime.now()-st)\n",
    "report_performance_on_test_sets(mnb_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6505cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!CHANGE ACCORDINGLY!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203b5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/MNB_scores.pkl', 'wb') as f:\n",
    "    pickle.dump(mnb_scores, f)\n",
    "        \n",
    "#with open('./data/MNB_scores.pkl', 'rb') as f:\n",
    "#    MNB_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3058abf",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_and_evaluate_model(model, param_space, X, Y):\n",
    "    \n",
    "    # define the pipeline\n",
    "    steps = list()\n",
    "    steps.append(('tfidf', TfidfVectorizer(tokenizer=LemmaTokenizer(), min_df=0.00125)))\n",
    "    steps.append(('model', model))\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    inner_cv = KFold(n_splits=2, shuffle=True, random_state=2023) # for hyperparameter tuning\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=2023)\n",
    "\n",
    "    #defining scoring for k fold cross validation\n",
    "    scoring = {\"F1 micro\": \"f1_micro\", \"Accuracy\": make_scorer(accuracy_score), \n",
    "               \"Hamming loss\": make_scorer(hamming_loss)}\n",
    "\n",
    "    # Nested CV with parameter optimization\n",
    "    # for  hypoparameter tuning f1 score will be used to find the best parameters for refitting the \n",
    "    # estimator at the end. It is specified with the parameter refit = 'F1 micro'.\n",
    "    clf = RandomizedSearchCV(pipeline, param_space, n_iter=3, scoring = scoring, cv = inner_cv, \n",
    "                             refit = 'F1 micro')\n",
    "    \n",
    "    scores = cross_validate(clf, X=X, y=Y, cv=outer_cv, return_estimator = True, scoring = scoring)\n",
    "\n",
    "    # report performance\n",
    "    report_performance_on_test_sets(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71523e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.now() \n",
    "\n",
    "clf_lr = OneVsRestClassifier(LogisticRegression(max_iter = 200))\n",
    "param_space = {\n",
    "              'model__estimator__C':[0.01,0.1,1,5,10] \n",
    "              }\n",
    "lr_scores = tune_and_evaluate_model(clf_lr, param_space, X, Y)\n",
    "\n",
    "print(\"Time taken to perform hyperparameter tuning and multiple model evaluation: \",datetime.now()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/LR_scores.pkl', 'wb') as g:\n",
    "    pickle.dump(lr_scores, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a0c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/LR_scores.pkl', 'rb') as g:\n",
    "#    lr_scores = pickle.load(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be051ba4",
   "metadata": {},
   "source": [
    "### Random forest classifier\n",
    "Inherently supports mulitilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6428d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st=datetime.now() \n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 2)\n",
    "\n",
    "param_space = {\n",
    "              'model__max_depth':list(np.arange(10, 250, step=50))\n",
    "              }\n",
    "rf_scores = tune_and_evaluate_model(rf, param_space, X, Y)\n",
    "print(\"Time taken to perform hyperparameter tuning and multiple model evaluation: \",datetime.now()-st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/RF_scores.pkl', 'wb') as h:\n",
    "    pickle.dump(rf_scores, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/RF_scores.pkl', 'rb') as h:\n",
    "#    RF_scores = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ffc59a",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "st=datetime.now() \n",
    "\n",
    "xgboost = xgb.XGBClassifier(objective = 'multi:softmax',                                  \n",
    "                            seed = 2023,  \n",
    "                            num_class=2,\n",
    "                            gamma =  0.1,\n",
    "                            learning_rate = 0.5,\n",
    "                            n_estimators = 200\n",
    "                            \n",
    "                         ) \n",
    "one_vs_rest_xgboost = OneVsRestClassifier(xgboost)\n",
    "\n",
    "param_space = {\n",
    "              'model__estimator__max_depth':list(np.arange(3,5, step=1))\n",
    "              }\n",
    "xgboost_scores = tune_and_evaluate_model(one_vs_rest_xgboost, param_space, X, Y)\n",
    "print(\"Time taken to perform hyperparameter tuning and multiple model evaluation: \",datetime.now()-st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460ebf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/XGBoost_scores.pkl', 'wb') as i:\n",
    "    pickle.dump(xgboost_scores, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e906cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./data/XGBoost_scores.pkl', 'rb') as i:\n",
    "#    XGBoost_scores = pickle.load(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81695d",
   "metadata": {},
   "source": [
    "## Evaluation of the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9907164",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_f1 =  {'Multinomial naive Bayes': mnb_scores['test_F1 micro'], \n",
    "                     'Logistic regression': lr_scores['test_F1 micro'],\n",
    "                     'Random forest': nrf_scores['test_F1 micro'],\n",
    "                     'XGBoost' : xgboost_scores['test_F1 micro']\n",
    "                    }\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(models_scores_f1.values(), showmeans=True, meanline = True)\n",
    "ax.set_xticklabels(models_scores_f1.keys())\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"F1 (micro) score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_accuracy = {'Multinomial naive Bayes': mnb_scores['test_Accuracy'], \n",
    "                 'Logistic regression': lr_scores['test_Accuracy'],\n",
    "                 'Random forest': nrf_scores['test_Accuracy'],\n",
    "                 'XGBoost' : xgboost_scores['test_Accuracy']\n",
    "                }\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(models_scores_accuracy.values(), showmeans=True, meanline = True)\n",
    "ax.set_xticklabels(models_scores_accuracy.keys())\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores_hamming = {'Multinomial naive Bayes': mnb_scores['test_Hamming loss'], \n",
    "                         'Logistic regression': lr_scores['test_Hamming loss'],\n",
    "                         'Random forest': nrf_scores['test_Hamming loss'],\n",
    "                         'XGBoost' : xgboost_scores['test_Hamming loss']\n",
    "                        }\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(models_scores_hamming.values(), showmeans=True, meanline = True)\n",
    "ax.set_xticklabels(models_scores_hamming.keys())\n",
    "ax.set_xlabel(\"Model\")\n",
    "ax.set_ylabel(\"Hamming loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
